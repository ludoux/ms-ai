### 第一题
>给定含有1000条记录的数据集mlm.csv，其中每条记录均包含两个自变量x,y和一个因变量z，它们之间存在较为明显的线性关系。

ref: 第5章 多入单出的单层神经网络 - 多变量线性回归

以简单数学的想法来说，二元一次线性方程（前向）出来即为

$$
z = x_1 \cdot w_1 + x_2 \cdot w_2 + b
$$

对于此神经网络而言，是一个神经元（有两个输入）。需要注意这里的 x_1 和 x_2 要理解成一个样本的两个特征值，而y输出则为这一个样本的标签值。因为线性所以没有激活函数。
公式
输入W 为2\*1，偏移B为1\*1。

损失：因为线性回归，所以均方差即可。
反向:

```python
class NeuralNet(object):
    def __backwardBatch(self, batch_x, batch_y, batch_z):
        m = batch_x.shape[0]
        dZ = batch_z - batch_y
        dB = dZ.sum(axis=0, keepdims=True)/m
        dW = np.dot(batch_x.T, dZ)/m
        return dW, dB
```

我（们）要先对数据进行归一，在深度学习中叫做“数据标准化”。具体原因在教程 05.3 中有。在本次代码中使用Min-Max标准化（离差标准化），将数据映射到 $[0,1]$ 区间。$$x_{new}=\frac{x-x_{min}}{x_{max} - x_{min}} \tag{1}$$

需要注意的是，不仅要对特征值做标准化（`NormalizeX`），也要对标签值做标准化（`NormalizeY`），最后出来结果的时候在对其做关于Y的反标准化即可。

![image-20210203221057143](/home/luu/Documents/Code/python/ai-homework/README.assets/image-20210203221057143.png)

![image-20210203221119675](/home/luu/Documents/Code/python/ai-homework/README.assets/image-20210203221119675.png)

```shell
finalloss:9.975988786386684e-06
W= [[ 0.53682978]
 [-0.48610054]]
B= [[0.48067892]]
W =  [[ 0.53682978]
 [-0.48610054]]
B =  [[0.48067892]]
```

出来的整体结果还是好的。

### 第二题

ref: 第11章 多入多出双层-非线性多分类

这里是使用两层神经网络，前向

#### 隐层1

$$Z1 = X \cdot W1 + B1 \tag{1}$$

$$A1 = Sigmoid(Z1) \tag{2}$$

#### 隐层2

$$Z2 = A1 \cdot W2 + B2 \tag{3}$$

$$A2 = Softmax(Z2) \tag{4}$$

![image-20210203225210637](/home/luu/Documents/Code/python/ai-homework/README.assets/image-20210203225210637.png)

```shell
===输出===
wb1.W =  [[ 0.61785025 -0.44138316 -0.31598824  0.83667877]
 [ 1.79203965 -1.93820474  1.76429771 -3.23871092]
 [-4.85648464  2.6215767  -0.19763024  4.49566358]
 [-5.50578593  4.20746238 -0.21293649  3.79599686]] 
wb1.B =  [[ 5.71947818 -3.1956158  -0.07191113 -1.43061498]] 
wb2.W =  [[ 3.35477329e+00 -8.87499870e+00  4.85806365e+00]
 [-1.89893367e+00  5.37463721e+00 -3.50054379e+00]
 [-1.44917368e+00  3.32134663e-03  1.46276405e+00]
 [ 3.33547758e+00  4.40640343e+00 -6.88767714e+00]] 
wb2.B =  [[-0.4491833  -0.99890469  1.44808799]]
```

